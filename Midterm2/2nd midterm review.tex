\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{textcomp}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}  % TODO: see page 94 of latex book
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{framed}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}



\title{CSCI 181 / E-181 Spring 2014 \\ 
{\large Midterm 2 Review Notes}
}
\author{
  David Wihl\\
  \texttt{davidwihl@gmail.com}
}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\section{Support Vector Machines}

\subsection{Background}
Characteristics of SVMs:
\begin{itemize}
	\item \emph{stock} -- SVMs are ``off the shelf" and ready to use. No special modification is necessary.
	\item \emph{linearly separable} -- assumes that linear separation is possible. Used natively as a binary classifier.
	\item \emph{convex optimization}. SVM originated as a backlash against neural nets due to nets' non-convexity. In Neural Nets, results were often non-reproducible as different researchers found different results due to different initializations.
	\item \emph{global optimum} -- SVMs will find the global optimum.
\end{itemize}

SVMs are based on three ``big ideas":
\begin{itemize}
	\item \emph{margin} Maximizes distance between the closest points
	\item \emph{duality} Take a hard problem and transform it into an easier problem to solve.
	\item \emph{kernel functions} map input vectors into higher dimensional, more expressive features to avoid costly computations.
\end{itemize}

\subsection{Definitions}

\begin{description}
	\item[Data:] $\{x_n,t_n\}_{n=1}^N, t_n \in \{-1, +1\}$. $t_n$ is the target or the expected result of the classification.
	\item[J Basis functions:] $\phi_j(x)\rightarrow\mathbb{R}$, therefore
	\begin{description}
		\item[Vector function:] $\Phi X \rightarrow \mathbb{R}^J$ produces a column vector.
	\end{description}
	\item[Objective function:]
$f(\textbf{x},\textbf{w},b) = \textbf{w}^\intercal \mathbf{\phi}(\textbf{x})  + b$
where b is the bias.
\end{description}

The sign of $f(\cdot)$ will determine classification $(-1,+1)$

So the actual classifier will be:
\[
y(\textbf{x},\textbf{w},b) =
	\begin{cases}
	  	+1, &\text{if  }\textbf{w}^\intercal \mathbf{\phi}(\textbf{x}) + b > 0 \\
		-1, &\text{otherwise}
	\end{cases}
\]

Unlike Logistic Regression (which uses $\{0, 1\}$), it is preferable to use $\{-1, +1\}$ as the classification result. If $t_n * y$ is positive, then the produced classification is correct (positive $\times$ positive is positive, negative $\times$ negative is also positive).

\emph{Decision Boundary} is the hyperplane where $\textbf{w}^\intercal \phi(\textbf{x}) +b = 0$ . We want to find the Decision Boundary that creates the most separation between the two different classes by maximizing the distance between the two closest points. The distance between the Decision Boundary and the closest point is called the \emph{margin}. The points closest to the Decision Boundary are called the \emph{support vectors}.

\subsection {Max Marginalization}

The margin is determined by the orthogonal distance from the closest point to the Decision Boundary: 
\begin{equation}
\frac{|\textbf{w}^\intercal \phi(\textbf{x}) + b|}{||\textbf{w}||}
\end{equation}


Maximizing the margin can be written as:
\begin{equation}
\argmax_{w,b} \left\{\min_n(t_n\cdot(\mathbf{w}^\intercal\phi(\mathbf{x})+b))\cdot\frac{1}{||w||} \right\}
\end{equation}

Maximizing the margin helps ensure that points which are close to margin will not be pushed over the boundary by noise.

$\mathbf{w}$ is orthogonal to vectors in the Decision Boundary. Here's how: pick two points on the Decision Boundary $\phi(x_1) \text{ and } \phi(x_2)$. So
\begin{align}
\mathbf{w}&^\intercal\left(\phi(x_2) - \phi(x_1)\right) = 0 \text{ for orthogonal dot product}\nonumber \\
\mathbf{w}&^\intercal\phi(x_2) - \mathbf{w}^\intercal\phi(x_1) = 0 \nonumber \\
\text{Note: } \mathbf{w}&^\intercal\phi(x_n) = (-b) \text{, so} \nonumber \\
=&(-b) - (-b) \nonumber \\
=& 0 \nonumber
\end{align}

Since $\mathbf{w}$ is orthogonal, we want to maximize it. It is not unit length, but could be, by scaling with a factor of $r$.

The margin is defined where
\begin{equation}
\mathbf{w}^\intercal\phi(x)+b = \pm 1
\end{equation}

The origin can be found at $\frac{b}{||w||}$.

See \href{http://cs229.stanford.edu/notes/cs229-notes3.pdf}{Stanford CS229 SVM Notes} re: Functional vs. Geometric Margins

The support vector is defined as $r\frac{\mathbf{w}}{||\mathbf{w}||_2}$, where $r$ is multiplied by the unit vector orthogonal to the Decision Boundary hyperplane. 

Define the point where the vector meets the plane as $\phi_\perp (\mathbf{x})$, so
\begin{equation}
\phi(\mathbf{x}) = \phi_\perp(\mathbf{x}) + r\frac{\mathbf{w}}{||\mathbf{w}||_2}
\end{equation}

Solving for $r$, multiple both sides by $\mathbf{w}^\intercal$.

(Recall: $\mathbf{w}^\intercal\mathbf{w} = ||\mathbf{w}||^2$)

\begin{align}
\mathbf{w}^\intercal\phi(\mathbf{x}) = &\mathbf{w}^\intercal  \phi_\perp(\mathbf{x})  +	 r\frac{\mathbf{w}^\intercal\mathbf{w}}{||\mathbf{w}||} \\
			 = & (-b) + r||\mathbf{w}||
\end{align}
Therefore, the margin for a point $\mathbf{x}$.
\begin{align}
r = &\frac{\phi(\mathbf{x})^\intercal\mathbf{w}+b}{||w||} \\
  = & \frac{f(\mathbf{x},\mathbf{w},b)}{||w||}
\end{align}
This makes it easy to calculate how far away a point is from the Decision Boundary. $r$ is strictly not a length because it could be negative. However, we only care about the actual distance to the boundary.

Margin for a datum $n$:
\begin{equation}
margin = t_n\frac{\phi(\mathbf{x})^\intercal\mathbf{w}+b}{||\mathbf{w}||}
\end{equation}

This is getting close to a loss function as we can now figure out the worst of these. The Margin for all the training data will be the point closest to the Decision Boundary:
\begin{equation}
min_n\left\{ t_n\frac{\phi(\mathbf{x})^\intercal\mathbf{w}+b}{||\mathbf{w}||} \right\}
\end{equation}

As mentioned at the beginning of this section, the Objective Function is 
\begin{equation}
\mathbf{w}^*,b^*=
\argmax_{w,b} \left\{\min_n(t_n\cdot(\phi(\mathbf{x})^\intercal\mathbf{w}+b))\cdot\frac{1}{||w||} \right\}
\end{equation}

but now we can simplify some things. $\mathbf{w}$ are $b$ are scale free (if we multiply by some $\beta$, the max and min will still be the same.)

Let's define a set of linear constraints such that the margin is always $\ge 1$ to make this easier to solve.
\begin{align}
\mathbf{w}^*,b^*= \argmax_{\mathbf{w},b}\frac{1}{||w||}
\end{align}
subject to 
\begin{align}
t_n\cdot(\phi(x_n)^\intercal\mathbf{w}+b) \ge 1 \:\forall\: n
\end{align}
This can be made even easier. Finding the max of $\frac{1}{||w||}$ is like finding the min of $||w||^2$, so
\begin{align}
\mathbf{w}^*,b^*=& \argmin_{\mathbf{w},b}||w||^2 \\
\text{s.t.} \:& t_n(\phi(x_n)^\intercal\mathbf{w} + b) \ge 1
\end{align}
so this reduces to just a quadratic program (QP) with linear constraints that could be solved by any number of commercial packages and produces a global minimum.

\subsection{Slack Variables}
If the data is not strictly linearly separable, it can mitigated by slack variables.

$\xi_n \leftarrow$ one for each datum. 
\begin{description}
 \item[$\xi_n = 0$] then the datum is correctly classified and outside the margin.
 \item[$0 < \xi_n \le 1$] the datum is correctly classified and within the margin
 \item[$\xi_n > 1$] the datum is misclassified
\end{description}

We will now add $\xi$ as a constraint to minimize.
\begin{equation}
t_n\cdot(\phi(x_n)^\intercal\mathbf{w}+b) \ge 1 - \xi_n
\end{equation}
New objective function:
\begin{align}
\mathbf{w}^*,b^*,\xi^*=& \argmin_{\mathbf{w},b,\mathbf{\xi}}\left\{||w||^2+ c \sum_{n=1}^{N}\xi_n\right\}\\
\text{s.t.} \:& t_n(\phi(x_n)^\intercal\mathbf{w} + b) \ge 1 -\xi_n\\
& \xi \ge 0\\
\forall \:n
\end{align}
where $c>0$ is the regularization parameter. A small $C$ means that you don't care much about errors. The sum of $\xi$ is the upper bound on how many can be wrong. If $c = 0$, it becomes the original function.
Typically,
\begin{equation}
c=\frac{1}{\nu N} \text{, where } 0 < \nu \le 1
\end{equation}
where $\nu$ is the tolerance for percentage willing to get wrong.

\subsection {Duality}
\begin{framed}
First a recap of Lagrange Multipliers.

Lagrange multipliers solve for $f(x,y,z)$ subject to $g(x,y,z) = k$ where $g(\cdot)$ is the \emph{constraint}.

Form:
\begin{equation}
F(x,y,z,\lambda) = f(x,y,z) - \lambda(g(x,y,z)-k)
\end{equation}
then solve for
$F_x = 0, F_y=0, F_z = 0, F_\lambda = 0$ using partial derivatives which will provide a max or min.
\end{framed}

Maximizing the margin:
\begin{align}
\mathbf{w}^*,b^*= &
\argmax_{w,b} \left\{\min_n(t_n\cdot(\mathbf{w}^\intercal\phi(\mathbf{x})+b))\cdot\frac{1}{||w||} \right\}\\
= &\argmax_{w,b} \frac{1}{||w||}\min_n(t_n\cdot(\mathbf{w}^\intercal\phi(\mathbf{x})+b))
\end{align}
This is still hard to differentiate. This is scalable by $\beta$.

We want to try a lot of basis functions. Duality allows us to try weights on \emph{data}, rather than basis functions. This allows an infinite number of dimensions. 

Solution: use duality. Associate a scalar with each constraint $\mathbf{\alpha} = [\alpha_1,\alpha_2,...,\alpha_N]$, each of which must be non-negative. Each constraint has it's own $\alpha$  which serves as the Lagrange multiplier for each constraint.
\begin{equation}
\mathcal{L}(\mathbf{w},b,\mathbf{\alpha}) = \frac{1}{2}||w||^2-\sum_{n=1}^N \alpha_n(t_n(\mathbf{w}^\intercal\phi(\mathbf{x})+b ) -1)
\end{equation}
The summation term will be negative if the constraint is violated. By throwing a minus sign in front and then maximizing $\alpha$ will make the term really large (maybe $\infty$). This will make $\min()$ unhappy - causing a huge value in the loss function. It is a math trick to show a constraint has been violated.
\begin{equation}
\mathbf{w}^*,b^*= \argmin_{\mathbf{w},b} \max_{\alpha \ge 0} \mathcal{L}(\mathbf{w},b,\mathbf{\alpha})
\end{equation}
The problem is now restated as a function of $\alpha$, not a $\mathbf{w},b$ problem. This is duality.

\emph{Weak duality}: the solution to an $\alpha$ problem makes the lower bounds of the w,b problem. Max $\alpha$ = min w,b. See formula 20 in maxmargin notes.

Strong duality: min and max can be switched without changing the answer. See formula 21 in maxmargin notes.
\begin{equation}
\frac{\partial L}{\partial \mathbf{w}} = 0 
\end{equation}

\begin{equation}
\frac{\partial L}{\partial b} = 0 
\end{equation}

$\alpha$'s are sparse so a lot of these terms will disappear. The remaining terms will provide the values as support vectors. Rewriting the Lagrangian just in terms of alpha can be found in formula 28 in maxmargin notes.

See formulas 28, 29 in maxmargin notes. This is a relatively easier problem to solve as it involves just the alphas.

Either alpha = 0, or $t_n(w^*\phi(x_n)+b^*) = 0$ These latter terms will be closest to the margin (they will be ``tightest"). A small subset of the data determine the boundary. Train on all of data and throw away most of the data in order to have the classifier.

The only time we need to compute something J dimensional is on the inner product. See $\phi$ term.

\subsection {Kernel Tricks}


``A kernel function is a scalar product on two vectors mapped by basis functions into a feature space. In general, we use kernels to map into higher dimensional feature spaces, using them to circumvent costly computations in high dimension spaces."

\begin{equation}
K(\mathbf{x},\mathbf{x}') = \phi^\intercal(\mathbf{x})\phi(\mathbf{x}')
\end{equation}

Kernel functions can be generalized to any distance measure. The larger K means the distance is closer. This can be applied to text, proteins, etc. (see Alpaydin example).

Exponentiate the negative squared distance of two dissimilar things, e.g.
\begin{equation}
K(x,z) = exp\left\{ -\frac{1}{2} ||x - z||^2 \right\}
\end{equation}

Don't engineer features - engineer distances. Feature space can be infinite (another way of saying this is that distances can be continuous as in a Gaussian distribution). Don't worry about features - worry about distance between things. Then create a kernel and use distance to discriminate.

Bayesian linear regression where all features interact as inner product so it can be turned into a kernel function. This can be a Gaussian kernel. Non-parametric infinite dimensional models using finite computers. Can also be used with PCA.

Mercer function, infinite dimensions (justification for duality). See also QR decomposition for large datasets.

Be cautious of pathological distance functions (as opposed to feature functions).

CS229 goes into a lot more depth about this: Gaussian kernels in depth, plus SMO and Karush-Kuhn-Tucker (KKT) conditions.

\subsection{Extensions to SVM}

SVM can be used for other purposes as well, such as regression. It can also be used for multi-class classification using \emph{one vs many}.

\subsection{Sources}

\begin{enumerate}
	\item Lecture 14, March 24, 2014
	\item Lecture 15, March 31, 2014
	\item Bishop 6.0-6.2 
	\item Bishop 7.0-7.1
	\item Course notes - maxmargin
	\item Section 7 review
	\item Section 8 review
	\item \href{http://cs229.stanford.edu/notes/cs229-notes3.pdf}{Stanford CS229 SVM notes}
	\item Machine Learning in Action, Chapter 6
\end{enumerate}

\section{Reinforcement Learning}

Online, related to planning, as opposed to the rigid data sets we've seen so far in regression and classification.

\subsection{Markov Decision Processes}

All of the following states, actions, transitions, rewards and policies look like functions, but in practice they are implemented as matrices in memory, given a finite set of actions and states.

States: $\mathcal{S} = \{1,2,3,...N\}$

Actions: $\mathcal{A} = \{1,2,3,...M\}$ which move us from state to state over a probability distribution. (You don't always land where you expected to go.). This noisy movement is defined by:

Transition Model $p(s'\mid s,a) \leftarrow$  PMF, indexed by state, action. These are a collection of $M$ stochastic matrices. Each matrix is $N \times N$, non-negative and all rows sum to 1. So each matrix $\mathbf{P}_{s,s'}^{(a)}$ where 
\begin{equation}
	P_{s,s'}^{(a)} \ge 0, \sum_{s'}P_{s,s'}^{(a)} = 1
\end{equation}

Reward Function: $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ Could be expressed as a $N \times M$ matrix, $\mathbf{\mathcal{R}}$

Policy: $\pi_t: \mathcal{S} \rightarrow \mathcal{A}$. \textbf{Finding the right policy is the objective here.} Array of length of $N$ containing integers between 1 and $M$. $\pi_s \in \{1,2,3,...M\}$.  (Visualize as a deck of cards, where each card is an action containing a matrix. Each row of the matrix is ``where you are" and each column is ``where you are going", which sends you off to the next action / card).

Similarly, the Value function $V(s)$ (see below) could be expressed as a vector $\mathbf{\mathcal{V}}_{(s)} \in \mathbb{R}^N$. In big state spaces (like a chess game), the value function has to be approximated. The approximation could be generated by something like a neural net to compress the current information about the state and next steps. (A chess game is not a fully observable Markov model).

Assumptions (for now -- these assumptions will be relaxed later):
\begin{enumerate}
	\item \emph{fully observable} You know what state you are in.
	\item \emph{known model}
	\item \emph{Markov Property} meaning the way the world works in the future is completely summarized by the current state. The past is not relevant. ``The future is independent of the past given the present."
	\item \emph{Finite actions and states}
	\item \emph{Bounded Rewards} - there is a maximum reward that dominates all rewards over all states.
\end{enumerate}

Agent could live forever (so it tries to maximize average reward) or lives a finite time (so it tries to maximize short term reward). Trade off of exploration vs. exploitation

Different types of utility ($u$) function:
\begin{description} \itemsep5pt
	\item[Finite Horizon*]	$u = \sum_{t=0}^{T-1}\mathcal{R}(s_t, a_t)$
	\item[Total Reward]$u = \sum_{t=0}^\infty\mathcal{R}(s_t,a_t)$ (may not converge)
	\item[Total Discounted Reward*]$u = \sum_{t=0}^\infty\gamma^t\mathcal{R}(s_t,a_t)$ where the discount factor $\gamma = [0,1)$
	\item[Long-run Average Reward]$u= \lim_{T\rightarrow\infty}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{R}(s_t,a_t)$
\end{description}

* Most common and tractable utility functions

Value means ``make a good move" in the anticipation of getting a reward.

\subsection{Expectimax}

Take an action to maximize utility. Nature responds randomly. For now, we know everything about everything except what nature will do.

We build the whole tree, work back the probabilities and then choose the path with the highest reward. The complexity is exponential -- such as $O(2^n)$ -- as it involves traversing a tree completely, depth-first. See Algorithm 1 in mdp Course Notes.

\subsection{Value Iteration}
Reward function: \emph{finite} - fixed set of rewards or \emph{infinite} - get as many rewards as possible with future rewards discounted compared to current rewards.

The Objective is to search the space of possible policies that maximizes rewards.

\subsubsection{Finite Horizon}

In the case of a finite horizon value iteration, start at the final state and work backwards.

Imagine the ``last gasp" policy, with one action left to take - pick the one that will maximize the reward.
\begin{align}
\pi_1(s) = &\argmax_a R(s,a)\\
V_1(s) = & \max_a R(s,a)
\end{align}
where 1 represents the number of steps left to take before the agent dies.

Working backwards one more step ``second to last gasp" - two actions left to take. In this case take into account the current reward and the next reward in the following step. 
\begin{align}
\pi_2(s) = & \argmax_a \left\{ R(s,a) + \sum_{s'} P(s'\mid s,a) \max_{a'} R(s',a') \right\}\\
V_2(s) = & \max_a \left\{ R(s,a) + \sum_{s'}p(s'\mid s,a) V_1(s')  \right\}
\end{align}
So generalizing with $K$ steps to go:
\begin{align}
V_k(s) =  &\max_a \left\{ R(s,a) + \sum_{s'} P(s'\mid s,a)V_{k-1}(s')  \right\}
\end{align}
$Q$-function = value of the (state, action) pair. Just like $V$ though it includes the action. If I know $Q$, then I know what action to take. Fix s, maximize over Q, and then choose the appropriate action to take.
\begin{align}
Q_k(s,a) = &R(s,a) + \sum_{s'} P(s'\mid s,a) V_{k-1}(s') \text{, where}\\
V_k(s) = & \max_a Q_k(s,a) = Q_k(s,\pi_k^*(s))\\
\pi_k^*(s) = & \argmax_a Q_k(s,a)
\end{align}

So now we can build a recursive implementation to find the best action for a given state. To make the recursion well defined, make sure that $V_0(s) = 0$. See Algorithm 2 pseudocode in mdp course notes. No need for a huge decision tree like in the Expectimax. Instead this is a simple table that is cheap and quick to populate.

\subsubsection{Infinite Horizon}

Here, we discount rewards for increasingly distant horizons. $\gamma$ is given as a parameter.
\begin{align}
u = & \sum_{t=0}^\infty\gamma^t R(s_t,a_t)\text{, where}\\
\gamma \in & (0,1)
\end{align}
Unlike finite horizon, in this case we move forward in time.
So the immediate reward (not $Q_k$, just $Q$ and just $\pi$):
\begin{align}
Q(s,a) = & R(s,a) + \gamma \sum_{s'} p(s' | s,a)V(s')\\
V(s) = & \max_a Q(s,a) = Q(s,\pi^*(s))\\
\pi^*(s) = &\argmax_a Q(s,a)
\end{align}
See CS229 Notes12 on Reinforcement Learning for a good discussion about this.

See policyiter class notes, algorithm 1.

\subsection{Policy Iteration}

Policy Iteration takes advantage of Linear Algebra to propagate information. Unlike Value Iteration, it does not need to wait until value convergence or the need to specify an $\varepsilon$. ``You don't really care about the value function - you care about what to do next."

See algorithm 2 in policyiter Class Notes.

Use $\pi$ as an index into the reward matrix to determine the next appropriate action.
\begin{align}
r_s^\pi = & R_{s,\pi_s}  \text{or as a vector:  } \\
\mathbf{r}^\pi \in & \mathbb{R}^N
\end{align}
Likewise, the policy could applied as an index into the transition matrix:
\begin{equation}
\mathbf{P}^\pi = \begin{bmatrix}
P_{1,1}^{(\pi_1)}  & P_{1,2}^{(\pi_1)} & \ldots & P_{1,N}^{(\pi_1)}\\[0.3em]
P_{2,1}^{(\pi_2)}  & P_{2,2}^{(\pi_2)} & \ldots & P_{2,N}^{(\pi_2)}\\[0.3em]
\vdots                   &                            & \ddots \\
P_{N,1}^{(\pi_N)}  & P_{N,2}^{(\pi_N)} & \ldots & P_{N,N}^{(\pi_N)}\\[0.3em]
\end{bmatrix}
\end{equation}

To compute a new value iteration:
\begin{align}
\mathbf{V} \leftarrow & \mathbf{r}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}\\
V(s) \leftarrow  & R(s,\pi(s)) + \gamma \sum_{s'} P(s' \mid s, \pi(s)) \times V(s')
\end{align}
This recursion is how the value propagates downs the chain, and allows us how to solve for $V$ in a fast and efficient way.
(This corresponds to policyiter class notes, algorithm 2, line 5, ``Solve system $V(s)$")
\begin{align}
&\mathbb{I} \mathbf{v} - \gamma \mathbf{P}^\pi\mathbf{v} = \mathbf{r}^\pi \\
&(\mathbb{I} - \gamma \mathbf{P}^\pi)\mathbf{v} = \mathbf{r}^\pi \\
&\mathbf{v} =  (\mathbb{I} - \gamma \mathbf{P}^\pi)^{-1}\mathbf{r}^\pi
\end{align}
Bad thing about Policy Iteration: solving the above matrix inverse is an $O(N^3)$ expensive operation. So each Policy Iteration is more expensive than Value Iteration, but in practice it is worth doing since it lets the information flow around the system is more efficiently than Value Iteration and will leverage common matrix libraries. (See discussion in policyiter, section 3 ``Comparing VI and PI").

\subsection{Reinforcement Learning}

(see RL Course Notes)

Relaxing of the previous assumptions, Reinforcement Learning is about discovering the world with an:
\begin{itemize} \itemsep5pt
	\item \textbf{Unknown Model}	$P(s' \mid s,a)$
	\item \textbf{Unknown Reward} $R(s,a)$
\end{itemize}
and then \textbf{plan} as these are discovered.

Trade off of \textbf{exploration} vs \textbf{exploitation}. How much time should an agent invest in learning the world vs using gained knowledge? Initially, more time would be spent in exploration. If the agent will die soon, it should spend more time exploiting.

Two types of RL:
\begin{itemize}
	\item \textbf{Model Based RL}. Try to understand the underlying dynamics of the world.
	\item \textbf{Model Free RL}. learn what to do without understanding the dynamics. Learn how to act. ``Don't solve a harder problem than the problem at hand." Can approximate the state space that are appealing.
\end{itemize}

\subsubsection{Model Based RL}

$N_{s,a}$: Track how often we took action $a$ in state $s$.

$R_{s,a}^{total}$ : total reward from doing $a$ in state $s$.

$N_{s,a,s'}$: transition model - how often did we ``land" in state $s'$ after taking action $a$ in state $s$.

Build up the model by taking these ratios:
\begin{align}
\hat{R}_{s,a} = & \frac{R_{s,a}^{total}} {N_{s,a}} \\
\hat{P}_{s,s'}^{(a)} = & \frac{N_{s,a,s'}} {N_{s,a}}
\end{align}
so we can now plan using Value or Policy Iteration.

To start, a prior could be seeded rather than 0 for the initial counts. That prior would be down weighted as evidence is collected.

Assumes that the reward is always coming from the same \emph{distribution} but not necessarily the same reward every time.

If we didn't explore, then these counts would stay small. Balance greed vs learning.

The most common method of balancing is $\varepsilon$ greedy. Most of the time, the optimal thing is done, but a certain $\varepsilon$ percentage of time exploration is performed. There should be a schedule for $\varepsilon$ so it decreases as everything has been explored.

Building the policies and doing planning is not free (recall $O(N^3)$). So a new policy or value iteration should not necessarily be done every action. Memoize the policy to save recalculating time.

\subsubsection{Model Free RL}

Typically based on \emph{Q-learning}, which tells you optimal action to take in any given state. Provides policy without understanding dynamics or rewards.

``Why store a model, when we can just keep asking `The World' for a noisy estimate? Kind of like a stochastic gradient descent."

From Bellman, we have a recursion.
\begin{align}
Q_{s,a} = &R_{s,a} + \gamma \sum_{s'} p(s' \mid s,a) \max_{a'} Q_{s',a'}\\
	      = & R_{s,a} + \gamma \mathbb{E}_{s'} \left[ \max_{a'} Q_{s',a'}\right]\\
	      = & \mathbb{E}_{s'} \left[ R_{s,a} + \gamma \max_{a'} Q_{s',a'}\right]
\end{align}

As the world gives us updated sample rewards and actions, they are used to update $Q$.
\begin{equation}
	Q_{s,a} \leftarrow Q_{s,a} + \alpha \left[ (r - \gamma \max_{a'} Q_{s',a'}) - Q_{s,a}\right]
\end{equation}
where:
\begin{description}
	\item[$r$] is the reward we actually got
	\item[$\alpha$] is a weight learning rate placed on the new values ($\in [0,1)$).
\end{description}

When dealing with very large state spaces, we can use a function approximator from functions of the state to get $Q$ values.  The approximator can be a linear regressor or any other function we've already seen

Assuming a huge state space: e.g. $s \in \mathbb{R}$, approximate $Q$:
\begin{equation}
\hat{Q}(s,a) = w_0^{(a)} + \phi(s)^\intercal \mathbf{w}^{(a)}
\end{equation}
We're just doing gradient descent on the $Q$'s so we can find the weights to provide a ``good enough" $Q$ function.
\subsection{Partially Observable MDP}

(Deepmind basically used Deep Learning + Model Free RL to solve their Atari game player).

\subsection{Mixture Models}

\subsection{Sources}

\begin{enumerate}
	\item Lecture 16, April 2, 2014 MDP
	\item Lecture 17, April 7, 2014 Value Iteration
	\item Lecture 18, April 9, 2014 Policy Iteration
	\item Lecture 19 POMDP
	\item Course notes - MDP
	\item Course notes - policyiter
	\item Course notes - RL
	\item Course notes - POMDP
	\item Bishop 9.0-9.2
	\item Section 9 review
	\item Section 10 review
	\item \href{http://cs229.stanford.edu/notes/cs229-notes12.pdf}{CS229 Reinforcement Learning and Control}
\end{enumerate}

\section{Expectation Maximization}

\subsection{Hidden Markov Models}

\subsection{Sources}

\begin{enumerate}
	\item Lecture 20 Mixture Models
	\item Lecture 21 Hidden Markov Models
	\item Bishop 9.3
	\item Bishop 13.0-13.2
	\item Section 11
	\item \href{http://cs229.stanford.edu/notes/cs229-notes7b.pdf}{CS229 Mixture of Gaussians}
	\item \href{http://cs229.stanford.edu/notes/cs229-notes8.pdf}{CS229 EM Notes}
\end{enumerate}

\end{document}  
